---
title: "Wie schÃ¼tzen Sie Ihr Unternehmen vor LLM-getriebenem Social Engineering und Shadow AI?"
date: 2025-12-06
layout: "page"
image: "page/images/2025-12-06-llm-security/hero.jpg"
summary: "LLM-Sicherheit, Prompt Injection und Shadow AI sind heute zentrale Risiken fÃ¼r Daten, Prozesse und Reputation. Dieses Whitepaper erklÃ¤rt anhand realer VorfÃ¤lle und aktueller Forschung, warum klassische Cybersecurity-Konzepte nicht ausreichen, welche technischen und organisatorischen MaÃŸnahmen State of the Art sind und wie ein pragmatisches Control Framework LLM- und Agenten-Nutzung in DACH-Unternehmen sicher macht. Zielgruppe: CIOs, CISOs und Innovationsmanager:innen."
include_footer: true
sidebar: true
categories: ["AI Sicherheit"]
---

{{< page-section >}}

{{< page-content >}}
# Warum ist LLM-getriebenes Social Engineering heute Ihre gefÃ¤hrlichste blinde Stelle? (Teil 1)

Stellen Sie sich einen Montagmorgen vor: Die Finanzchefin bittet den Copilot um eine Zusammenfassung wichtiger Mails zum M&A-Deal. Sekunden spÃ¤ter flieÃŸen hochsensible Informationen Ã¼ber einen versteckten Bildlink an einen externen Server â€“ ausgelÃ¶st durch eine prÃ¤parierte Eâ€‘Mail.

Dieses Zeroâ€‘Click-Szenario wurde in der EchoLeakâ€‘Analyse fÃ¼r Microsoft 365 Copilot beschrieben [1]. Dort zeigt sich, wie indirekte Prompt Injection Eâ€‘Mails, Dateien und Plugins kombinieren kÃ¶nnen, sodass ein Assistent seine Vertrauensgrenzen Ã¼berschreitet.
{{< /page-content >}}

{{< page-outline >}}
> â„¹ï¸ Fokus dieser EinfÃ¼hrung
> - Kurzes, greifbares Szenario mit einem kompromittierten Copilot.
> - EchoLeak als praktisches Beispiel fÃ¼r Zeroâ€‘Click Prompt Injection [1].
> - Ziel: Aufmerksamkeit fÃ¼r das Risiko, das aus der Interaktion von LLM, Daten und Aktionen entsteht.
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Warum ist LLM-getriebenes Social Engineering heute Ihre gefÃ¤hrlichste blinde Stelle? (Teil 2)

Forschende weisen seit 2023 darauf hin, dass indirekte Prompt Injectionsâ€”versteckte Anweisungen in Dokumenten, Webseiten oder Mailsâ€”LLMâ€‘integrierte Anwendungen systematisch kompromittieren kÃ¶nnen [2]. OWASP fÃ¼hrt Prompt Injection inzwischen als Topâ€‘Risiko fÃ¼r LLMâ€‘Anwendungen [3].

Parallel professionalisieren sich AIâ€‘gestÃ¼tzte Socialâ€‘Engineeringâ€‘Kampagnen: mehrsprachige Phishingâ€‘Mails, synthetische Stimmen und gefÃ¤lschte Chats. Angreifer zielen direkt auf Ihre Assistenten und Automatisierungsâ€‘Workflows.
{{< /page-content >}}

{{< page-outline >}}
> ðŸ’¡ Kernaussage
> - Treat every externe Eingabe als potenziell feindlich.
> - LLMs verschieben Social Engineering weg vom Menschen hin zu KIâ€‘Assistenten.
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Warum reichen klassische Cybersecurityâ€‘Konzepte fÃ¼r LLMâ€‘Anwendungen nicht mehr aus? (Teil 1)

Traditionelle Architekturen trennen Zonen mit Firewalls, regeln Zugriffe Ã¼ber IAM und filtern Eâ€‘Mails. Moderne LLMâ€‘Assistenten brechen diese Trennungen auf: Sie sind Ãœbersetzer, Orchestrator und Interface zu internen und externen Quellen zugleich. Prompt Injection nutzt diese Vermischung aus, weil LLMs Inhalte und Anweisungen oft nicht sauber trennen [2][3].
{{< /page-content >}}

{{< page-outline >}}
âœ“ Dos & âœ— Don'ts

âœ“ Dos & âœ— Don'ts
- âœ“ ErklÃ¤re, warum LLMs eine andere Sicherheitslogik benÃ¶tigen als Web oder Mobile.
- âœ“ Mache Prompt Injection fÃ¼r Entscheider:innen verstÃ¤ndlich.
- âœ— Verliere dich nicht in Modellarchitekturâ€‘Details.
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Warum reichen klassische Cybersecurityâ€‘Konzepte fÃ¼r LLMâ€‘Anwendungen nicht mehr aus? (Teil 2)

Ein weiterer Treiber ist Shadow AI: Fachbereiche rollen PoCs ohne Security, Legal oder Compliance aus. So entstehen produktive LLMâ€‘Workflows ohne Risikoanalyse, Trust Boundaries oder klare Policies â€” eine ideale AngriffsflÃ¤che fÃ¼r Social Engineering und Datenabfluss.
{{< /page-content >}}

{{< page-outline >}}
> â„¹ï¸ Governanceâ€‘Problem
> - Shadow AI entsteht meist durch Innovationsdruck in Fachbereichen.
> - Sicherheitsâ€‘ und Complianceâ€‘Teams werden oft zu spÃ¤t eingebunden.
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Reale Bedrohungslandschaft fÃ¼r LLMâ€‘Sicherheit und AIâ€‘Agenten: Prompt Injection

Prompt Injection und Kontextmanipulation sind zentrale Gefahren: Versteckte Anweisungen in Webseiten oder Dokumenten kÃ¶nnen Antworten manipulieren, Daten abflieÃŸen lassen oder Aktionen auslÃ¶sen. Forschungsarbeiten zeigen diese Angriffe bereits seit 2023 in Prototypen und realen Umgebungen [2][1].
{{< /page-content >}}

{{< page-outline >}}
> ðŸ’¡ Kernelemente
> - Prompt Injection zielt auf die Schnittstelle Modellâ†”Datenâ†”Aktionen.
> - EchoLeak demonstriert Zeroâ€‘Clickâ€‘Risiken in Enterpriseâ€‘Systemen [1].
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Reale Bedrohungslandschaft: Beispiele aus Forschung und Praxis

PraxisfÃ¤lle belegen die Risiken: Blue41 dokumentiert, wie Markenchatbots per Prompt Injection zu falschen Preisen, beleidigenden Antworten oder fehlerhaften AGBâ€‘Angaben verleitet wurden [5]. Solche VorfÃ¤lle kÃ¶nnen rechtliche und finanzielle Folgen haben.
{{< /page-content >}}

{{< page-outline >}}
> â„¹ï¸ Praxisbeispiel
> - Blue41 beschreibt reale Promptâ€‘Injectionâ€‘VorfÃ¤lle an Kundenchatbots [5].
> - Konsequenz: SchÃ¤den fÃ¼r Reputation, Recht und Finanzen.
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Reale Bedrohungslandschaft: Modellâ€‘Schwachstellen

Wiederholte Tokenâ€‘Sequenzen und spezialisierte Eingaben bringen generative Modelle in Divergenzmodi, in denen sie memorisierte Trainingsdaten wiedergeben. Dropbox dokumentiert solche Effekte und stellt Codeâ€‘Beispiele und Analysen bereit [6][7].
{{< /page-content >}}

{{< page-outline >}}
> ðŸ’¡ Hinweis
> - Modellâ€‘Verhalten kann zu unbeabsichtigten Datenleaks fÃ¼hren.
> - Tests und Fuzzing sind notwendig, um solche Schwachstellen zu finden.
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Reale Bedrohungslandschaft: AIâ€‘gestÃ¼tztes Social Engineering

LLMs senken HÃ¼rden fÃ¼r glaubwÃ¼rdige Phishingâ€‘Kampagnen, CEOâ€‘Fraud und Businessâ€‘Emailâ€‘Compromise. Angriffe kombinieren generierte Texte mit Deepfakes oder gefÃ¤lschten Dialogen, um Freigaben oder Zahlungen zu erzwingen. Die AngriffsÃ¶konomie verÃ¤ndert sich dadurch grundlegend.
{{< /page-content >}}

{{< page-outline >}}
> â„¹ï¸ Trend
> - Automatisierung reduziert Kosten und erhÃ¶ht Skalierbarkeit von Socialâ€‘Engineeringâ€‘Kampagnen.
> - SchutzmaÃŸnahmen mÃ¼ssen Mensch, Prozess und Technik kombinieren.
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Welche typischen Fehler befeuern Shadow AI und unsichere LLMâ€‘Nutzung? (Teil 1)

In DACHâ€‘Unternehmen wiederholen sich typische Muster:

- LLMs wie â€žbessere Sucheâ€œ behandeln und sensible Dokumente ungefiltert nutzen.
- Keine Trennung zwischen vertrauenswÃ¼rdigen und untrusted Quellen in RAGâ€‘Pipelines.
- PoCs ohne Logging, Rollenmodelle oder Red Teaming unternehmensweit ausrollen.
{{< /page-content >}}

{{< page-outline >}}
> â„¹ï¸ Typische Muster
> - Fehlende Datenklassifikation und Maskierung.
> - RAGâ€‘Pipelines, die interne und externe Quellen mischen.
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Welche typischen Fehler befeuern Shadow AI und unsichere LLMâ€‘Nutzung? (Teil 2)

Weitere Fehler:

- Shadow AI: Mitarbeitende nutzen private Accounts oder Selfâ€‘Hosted Modelle ohne zentrale Kontrollmechanismen.
- Unrealistische Erwartungen an System Prompts und â€žsichereâ€œ Modellvarianten. Reale Angriffe belegen deren Grenzen [5][1].
{{< /page-content >}}

{{< page-outline >}}
> ðŸ’¡ Wirkung
> - Diese Muster erlauben LLMs den Zugriff auf kritische Prozesse ohne systematische Risikoanalyse.
> - Ergebnis: erhÃ¶hte Gefahr fÃ¼r Datenverlust und Complianceâ€‘BrÃ¼che.
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Rahmenwerke und Standards fÃ¼r sichere LLMâ€‘Architekturen (Teil 1)

Vorhandene Frameworks bieten Orientierung:

- OWASP LLMâ€‘Topâ€‘Risiken als technische Checkliste [3].
- NIST AI Risk Management Framework fÃ¼r Governance, DatenqualitÃ¤t und Monitoring; in EchoLeakâ€‘Analysen wird NIST zur Bewertung herangezogen [1].
{{< /page-content >}}

{{< page-outline >}}
> ðŸ’¡ Praktische Nutzung
> - OWASP als technische Mindestbasis.
> - NIST verbindet Cybersecurity, Datenschutz und GeschÃ¤ftsrisiken.
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Rahmenwerke und Standards fÃ¼r sichere LLMâ€‘Architekturen (Teil 2)

Weitere Hinweise:

- Branchenempfehlungen (z. B. Globant) raten, Trust Boundaries und Missbrauchspfade explizit zu modellieren [8].
- EU AI Act verlangt fÃ¼r Hochrisikoâ€‘AI Dokumentation, Transparenz und Security by Design. FrÃ¼he Investitionen reduzieren spÃ¤teren Complianceâ€‘Aufwand.
{{< /page-content >}}

{{< page-outline >}}
> â„¹ï¸ Umsetzungstipp
> - Integrieren Sie Anforderungen des EU AI Act in Ihr ISMS.
> - Nutzen Sie Branchenâ€‘Guidance als konkrete Architekturâ€‘Checkliste [8].
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Technische SchutzmaÃŸnahmen gegen Prompt Injection und Datenabfluss (Teil 1)

Erfolgreiche AnsÃ¤tze folgen dem Prinzip Defenseâ€‘inâ€‘Depth. Wichtige technische Patterns:

- Prompt Partitioning: System Prompt, Nutzeranfrage und externe Inhalte streng trennen.
- Provenanceâ€‘basiertes Retrieval: Quellen nach SensitivitÃ¤t und Freigabe filtern.
- Input/Outputâ€‘Filter: Heuristiken und modellgestÃ¼tzte Detection stoppen typische Injectionâ€‘Muster [6][7].
{{< /page-content >}}

{{< page-outline >}}
âœ“ Dos & âœ— Don'ts

âœ“ Dos & âœ— Don'ts
- âœ“ Betone Defense in Depth.
- âœ“ Nenne konkrete Patterns, die Architekt:innen nutzen kÃ¶nnen.
- âœ— Versprich nicht, Prompt Injection vollstÃ¤ndig zu eliminieren.
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Technische SchutzmaÃŸnahmen gegen Prompt Injection und Datenabfluss (Teil 2)

Weitere MaÃŸnahmen:

- Strikte Contentâ€‘Securityâ€‘Policies und Egressâ€‘Kontrolle; Renderâ€‘OberflÃ¤chen dÃ¼rfen keine externen Skripte laden.
- Red Teaming und automatisiertes Fuzzing (Tools wie Garak, Hou Yi, PromptMap) identifizieren Schwachstellen vor dem Produktiveinsatz [8][6].
{{< /page-content >}}

{{< page-outline >}}
> â„¹ï¸ Technikâ€‘Fokus
> - Kontrollieren Sie ausgehende Verbindungen zentral Ã¼ber Proxies und Allowlists.
> - Automatisierte Tests sollten Teil der CI/CDâ€‘Pipelines sein.
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Smart AI Sparks Control Framework: Zielbild fÃ¼r DACH Unternehmen

Ein pragmatisches Control Framework umfasst:

- Zentrales AI Control Plane fÃ¼r alle LLMâ€‘Zugriffe mit Logging und Policyâ€‘Enforcement.
- Kontextâ€‘Aware Policies statt pauschaler Verbote: Datenklassen, RAGâ€‘Quellen und Agentenâ€‘Befugnisse definieren.
- Security by Design, Selfâ€‘Service Kataloge und kontinuierliches Adversarial Testing.
{{< /page-content >}}

{{< page-outline >}}
> â„¹ï¸ Zielbild
> - Sicherheit, Governance und Innovation in einer gemeinsamen Schicht.
> - Shadow AI wird reduziert, weil sichere, attraktive Alternativen bereitstehen.
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Welche nÃ¤chsten Schritte sollten Sie als CIO oder CISO morgen einleiten? (Roadmap 90 Tage â€” Teil 1)

1. Istâ€‘Aufnahme: Inventar aller sichtbaren und vermuteten LLMâ€‘Nutzungen (Copilots, Chatbots, PoCs, Plugins).

2. Schnellcheck: Grobe Bewertung Ihrer Topâ€‘3 AIâ€‘Useâ€‘Cases gegen OWASP und NISTâ€‘Kriterien [3][1].

3. AI Governance Board: Kleines, handlungsfÃ¤higes Gremium aus IT, Security, Legal und Fachbereichen bilden.
{{< /page-content >}}

{{< page-outline >}}
> ðŸ’¡ Aktionsfokus
> - Beginnen Sie mit Inventar und Schnellcheck.
> - Setzen Sie ein kleines Governanceâ€‘Board fÃ¼r schnelle Entscheidungen.
{{< /page-outline >}}

{{< /page-section >}}

{{< page-section >}}

{{< page-content >}}
# Welche nÃ¤chsten Schritte sollten Sie als CIO oder CISO morgen einleiten? (Roadmap 90 Tage â€” Teil 2)

4. Pilot AI Gateway: Starten Sie ein Pilotprojekt mit zentralem Gateway, Logging, Policyâ€‘Enforcement und Basisschutz.

5. Sensibilisierung: ErgÃ¤nzen Sie Security Awareness um AIâ€‘Socialâ€‘Engineeringâ€‘Szenarien und messen Sie Wirkung.
{{< /page-content >}}

{{< page-outline >}}
> â„¹ï¸ Erwarteter Nutzen
> - Schnelle Risikominderung fÃ¼r priorisierte Useâ€‘Cases.
> - Grundlage schaffen, um LLMs sicher in kritische Prozesse zu integrieren.
{{< /page-outline >}}

{{< /page-section >}}
{{< page-cta image="page/images/cta.png" alt="Jetzt starten" button-text="Jetzt unverbindlich anfragen" button-link="/contact" >}}
Jetzt starten: BÃ¼ndeln Sie Ihre LLMâ€‘Initiativen unter einem gemeinsamen Sicherheitsâ€‘ und Governanceâ€‘Dach.

- Benennen Sie eine verantwortliche Person fÃ¼r "AI Sicherheit und Governance" mit direkter Berichtslinie an CIO oder CISO.
- Initiieren Sie einen 2â€“3â€‘stÃ¼ndigen Workshop mit IT, Security, Legal und zwei Fachbereichen, um die 90â€‘Tageâ€‘Roadmap anzupassen.
- Priorisieren Sie zwei LLMâ€‘Useâ€‘Cases fÃ¼r einen ersten Pilot mit AIâ€‘Gateway, Promptâ€‘Injectionâ€‘Schutz und Monitoring.
- Vereinbaren Sie MessgrÃ¶ÃŸen: reduzierte Shadow AI, Anzahl durchlaufener Architekturâ€‘Reviews, Zeit bis zur sicheren Produktivsetzung.
{{< /page-cta >}}
{{< page-section >}}

{{< page-content >}}
## Quellen

1. [EchoLeak 2025 â€“ Zeroâ€‘click prompt injection in Microsoft 365 Copilot](https://arxiv.org/html/2509.10540v1/)  
2. [Greshake et al. 2023 â€“ Indirect Prompt Injection Paper](https://www.arxiv.org/abs/2302.12173?context=cs.AI)  
3. [OWASP â€“ LLM Risks and Guidance](https://en.wikipedia.org/wiki/Jailbreaking_(AI))  
4. [TechTarget 2024 â€“ Zenity CTO on Copilot prompt injections](https://www.techtarget.com/searchsecurity/news/366602358/Zenity-CTO-on-dangers-of-Microsoft-Copilot-prompt-injections)  
5. [Blue41 2024 â€“ Real world attacks on LLM applications](https://blue41.cs.kuleuven.be/blog/real-world-attacks-on-llm-applications/)  
6. [Dropbox 2024 â€“ Token attack analysis on ChatGPT models](https://dropbox.tech/machine-learning/bye-bye-bye-evolution-of-repeated-token-attacks-on-chatgpt-models)  
7. [Dropbox GitHub â€“ llm security Repository](https://github.com/dropbox/llm-security)  
8. [Globant 2024 â€“ Securing LLMs against prompt injection](https://stayrelevant.globant.com/en/technology/cybersecurity/prompt-injection-explained/)
{{< /page-content >}}

{{< page-outline image="page/images/references.png" >}}

{{< /page-outline >}}

{{< /page-section >}}